{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4215e97c-a840-4107-a9c6-926f71a1a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2126c864-9d18-4711-88f7-5c2f35865d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding1D(nn.Module):\n",
    "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, dim):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Input has shape `(batch_size, seq_len, emb_dim)`\"\"\"\n",
    "        # (1, 3, 14, 14) + (1, 3, 196) ? wtf\n",
    "        # ohhh flatten before embedding kek\n",
    "        return x + self.pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cc454042-e611-4cae-8210-f51dc1c3da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc2(F.gelu(self.fc1(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "168fb4ea-14a8-4aba-bef9-e7e48633004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.project = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x): # input shape [b, s, d]\n",
    "        # split into q, k, v : (query, key, values)\n",
    "        # expand by h (num heads... hence multi attention)\n",
    "        \n",
    "        q, k, v = self.project(x), self.project(x), self.project(x)\n",
    "        # convert q, k, v -> [b, h, s, w] where h = n_heads\n",
    "        q = q.view(q.shape[0], self.n_heads, q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], self.n_heads, k.shape[1], -1)\n",
    "        v = v.view(v.shape[0], self.n_heads, v.shape[1], -1)\n",
    "        \n",
    "        # scaled dot product attention on q, k (queries, keys) then matmul with values\n",
    "        # matmul + scale\n",
    "        # [b, h, s, w] @ [b, h, w, s] -> [b, h, s, s]\n",
    "        k = k.transpose(-2, -1) # swap last two dimensions\n",
    "        p = torch.matmul(q, k)\n",
    "        p = p / np.sqrt(k.size(-1)) # where s is the dimension of k\n",
    "\n",
    "        p = F.softmax(p, dim = -1) # softmax across last dimension\n",
    "        \n",
    "        out = torch.matmul(p, v) # [b, h, s, s] @ [b, h, s, w] -> [b, h, s, w]\n",
    "        out = out.view(out.shape[0], out.shape[2], -1) # [b, s, d]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0de52869-4ffa-4be1-b875-c16b18af1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need linear and dont need dropout i think\n",
    "# add after if performance is shit\n",
    "\n",
    "class Block(nn.Module): # inputs are B, S, D\n",
    "    def __init__(self, dim, n_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.MHSA = MHSA(dim, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mlp_channels = MLP(dim, ff_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.MHSA(self.norm1(x))\n",
    "        x = x + h\n",
    "        h = self.mlp_channels(self.norm2(x))\n",
    "        x = x + h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a7beb667-2fc6-4786-bbc6-3c3960a3e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers, dim, n_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim, n_heads, ff_dim) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "714c0e83-2177-43fe-90f1-acdbb24117e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith image 1, 3, 224, 224  and patch encoding of 16x16 we have\\n((224 - 16) / 16) + 1 = 14\\nseq len is 14 * 14 bro im trippin bullets\\nout = (1, 3, 14, 14)\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with image 1, 3, 224, 224  and patch encoding of 16x16 we have\n",
    "((224 - 16) / 16) + 1 = 14\n",
    "seq len is 14 * 14 bro im trippin bullets\n",
    "out = (1, 3, 14, 14)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c1e746a-e682-476f-87ec-58a6024bd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels, dim, fh, fw, n_layers, n_heads, ff_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fw = fw\n",
    "        self.fh = fh\n",
    "        self.patch_encoding = nn.Conv2d(in_channels, dim, kernel_size=(self.fh, self.fw), stride=(self.fh, self.fw))\n",
    "        # [B, D, FH, FW]\n",
    "        \n",
    "        seq_len = 14 * 14\n",
    "            \n",
    "        self.positional_embedding = PositionalEmbedding1D(seq_len, dim) # inputs are seq len, dim\n",
    "        # [B, D, FH, FW]\n",
    "        \n",
    "        # flatten into [B, S, D]\n",
    "        self.Transformer = Transformer(n_layers, dim, n_heads, ff_dim)\n",
    "        \n",
    "        # if this doenst work then do it manually\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_encoding(x)\n",
    "        x = x.view(x.shape[0], -1, x.shape[1]) # b, s, d\n",
    "        x = self.positional_embedding(x) \n",
    "        x = self.Transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, -1, :] # b, s, d -> b, d\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64fe747f-dcc7-45ed-b95c-b62ebaa8b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "fw = fh = 16\n",
    "dim = 768\n",
    "ff_dim = 3072\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "in_channels = 3\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "439d23fb-45ac-41a4-80d5-b8a5ec397edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(in_channels, dim, fh, fw, n_layers, n_heads, ff_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "55bec6c0-b353-4c88-80c2-392f1eedfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input of (1, 3, 224, 224) like in research paper\n",
    "x = torch.rand(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fd0b6d98-ac55-4faa-9d68-91c4adb0d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "253459c7-1453-4fb3-bae1-32359f426319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4575,  0.1935,  0.5040, -0.1220,  0.4996,  0.4516, -0.5189, -0.4725,\n",
       "          0.0136, -0.0068]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc6bbcd6-47f1-4f19-8d6d-275d1d57652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"b16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d351ac2-875a-403a-9767-596becc9926f",
   "metadata": {},
   "source": [
    "### I NEED TO APPEND Transformer.xyz to every block or else it wont load the damn weight fml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89d010-bce9-44c7-ae43-a96c37724c5b",
   "metadata": {},
   "source": [
    "## figure out what each key is and rename my model to match... smh\n",
    "* blocks.n.norm1.bias1 -> transformer blocks (class Block) DO THIS FOR 12 BLOCKS\n",
    "* blocks.mlp_channels \n",
    "    * change mlp to mlp_channels \n",
    "        * change l1, l2 to fc1, fc2\n",
    "    * change self.block to self.blocks\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453f6e3-c279-4d62-955f-5d17d6757fe7",
   "metadata": {},
   "source": [
    "odict_keys(['blocks.0.norm1.bias', 'blocks.0.norm1.weight', 'blocks.0.norm2.bias', 'blocks.0.norm2.weight', 'blocks.0.mlp_channels.fc1.bias', 'blocks.0.mlp_channels.fc1.weight', 'blocks.0.mlp_channels.fc2.bias', 'blocks.0.mlp_channels.fc2.weight', 'blocks.0.mlp_tokens.fc1.bias', 'blocks.0.mlp_tokens.fc1.weight', 'blocks.0.mlp_tokens.fc2.bias', 'blocks.0.mlp_tokens.fc2.weight', 'blocks.1.norm1.bias', 'blocks.1.norm1.weight', 'blocks.1.norm2.bias', 'blocks.1.norm2.weight', 'blocks.1.mlp_channels.fc1.bias', 'blocks.1.mlp_channels.fc1.weight', 'blocks.1.mlp_channels.fc2.bias', 'blocks.1.mlp_channels.fc2.weight', 'blocks.1.mlp_tokens.fc1.bias', 'blocks.1.mlp_tokens.fc1.weight', 'blocks.1.mlp_tokens.fc2.bias', 'blocks.1.mlp_tokens.fc2.weight', 'blocks.10.norm1.bias', 'blocks.10.norm1.weight', 'blocks.10.norm2.bias', 'blocks.10.norm2.weight', 'blocks.10.mlp_channels.fc1.bias', 'blocks.10.mlp_channels.fc1.weight', 'blocks.10.mlp_channels.fc2.bias', 'blocks.10.mlp_channels.fc2.weight', 'blocks.10.mlp_tokens.fc1.bias', 'blocks.10.mlp_tokens.fc1.weight', 'blocks.10.mlp_tokens.fc2.bias', 'blocks.10.mlp_tokens.fc2.weight', 'blocks.11.norm1.bias', 'blocks.11.norm1.weight', 'blocks.11.norm2.bias', 'blocks.11.norm2.weight', 'blocks.11.mlp_channels.fc1.bias', 'blocks.11.mlp_channels.fc1.weight', 'blocks.11.mlp_channels.fc2.bias', 'blocks.11.mlp_channels.fc2.weight', 'blocks.11.mlp_tokens.fc1.bias', 'blocks.11.mlp_tokens.fc1.weight', 'blocks.11.mlp_tokens.fc2.bias', 'blocks.11.mlp_tokens.fc2.weight', 'blocks.2.norm1.bias', 'blocks.2.norm1.weight', 'blocks.2.norm2.bias', 'blocks.2.norm2.weight', 'blocks.2.mlp_channels.fc1.bias', 'blocks.2.mlp_channels.fc1.weight', 'blocks.2.mlp_channels.fc2.bias', 'blocks.2.mlp_channels.fc2.weight', 'blocks.2.mlp_tokens.fc1.bias', 'blocks.2.mlp_tokens.fc1.weight', 'blocks.2.mlp_tokens.fc2.bias', 'blocks.2.mlp_tokens.fc2.weight', 'blocks.3.norm1.bias', 'blocks.3.norm1.weight', 'blocks.3.norm2.bias', 'blocks.3.norm2.weight', 'blocks.3.mlp_channels.fc1.bias', 'blocks.3.mlp_channels.fc1.weight', 'blocks.3.mlp_channels.fc2.bias', 'blocks.3.mlp_channels.fc2.weight', 'blocks.3.mlp_tokens.fc1.bias', 'blocks.3.mlp_tokens.fc1.weight', 'blocks.3.mlp_tokens.fc2.bias', 'blocks.3.mlp_tokens.fc2.weight', 'blocks.4.norm1.bias', 'blocks.4.norm1.weight', 'blocks.4.norm2.bias', 'blocks.4.norm2.weight', 'blocks.4.mlp_channels.fc1.bias', 'blocks.4.mlp_channels.fc1.weight', 'blocks.4.mlp_channels.fc2.bias', 'blocks.4.mlp_channels.fc2.weight', 'blocks.4.mlp_tokens.fc1.bias', 'blocks.4.mlp_tokens.fc1.weight', 'blocks.4.mlp_tokens.fc2.bias', 'blocks.4.mlp_tokens.fc2.weight', 'blocks.5.norm1.bias', 'blocks.5.norm1.weight', 'blocks.5.norm2.bias', 'blocks.5.norm2.weight', 'blocks.5.mlp_channels.fc1.bias', 'blocks.5.mlp_channels.fc1.weight', 'blocks.5.mlp_channels.fc2.bias', 'blocks.5.mlp_channels.fc2.weight', 'blocks.5.mlp_tokens.fc1.bias', 'blocks.5.mlp_tokens.fc1.weight', 'blocks.5.mlp_tokens.fc2.bias', 'blocks.5.mlp_tokens.fc2.weight', 'blocks.6.norm1.bias', 'blocks.6.norm1.weight', 'blocks.6.norm2.bias', 'blocks.6.norm2.weight', 'blocks.6.mlp_channels.fc1.bias', 'blocks.6.mlp_channels.fc1.weight', 'blocks.6.mlp_channels.fc2.bias', 'blocks.6.mlp_channels.fc2.weight', 'blocks.6.mlp_tokens.fc1.bias', 'blocks.6.mlp_tokens.fc1.weight', 'blocks.6.mlp_tokens.fc2.bias', 'blocks.6.mlp_tokens.fc2.weight', 'blocks.7.norm1.bias', 'blocks.7.norm1.weight', 'blocks.7.norm2.bias', 'blocks.7.norm2.weight', 'blocks.7.mlp_channels.fc1.bias', 'blocks.7.mlp_channels.fc1.weight', 'blocks.7.mlp_channels.fc2.bias', 'blocks.7.mlp_channels.fc2.weight', 'blocks.7.mlp_tokens.fc1.bias', 'blocks.7.mlp_tokens.fc1.weight', 'blocks.7.mlp_tokens.fc2.bias', 'blocks.7.mlp_tokens.fc2.weight', 'blocks.8.norm1.bias', 'blocks.8.norm1.weight', 'blocks.8.norm2.bias', 'blocks.8.norm2.weight', 'blocks.8.mlp_channels.fc1.bias', 'blocks.8.mlp_channels.fc1.weight', 'blocks.8.mlp_channels.fc2.bias', 'blocks.8.mlp_channels.fc2.weight', 'blocks.8.mlp_tokens.fc1.bias', 'blocks.8.mlp_tokens.fc1.weight', 'blocks.8.mlp_tokens.fc2.bias', 'blocks.8.mlp_tokens.fc2.weight', 'blocks.9.norm1.bias', 'blocks.9.norm1.weight', 'blocks.9.norm2.bias', 'blocks.9.norm2.weight', 'blocks.9.mlp_channels.fc1.bias', 'blocks.9.mlp_channels.fc1.weight', 'blocks.9.mlp_channels.fc2.bias', 'blocks.9.mlp_channels.fc2.weight', 'blocks.9.mlp_tokens.fc1.bias', 'blocks.9.mlp_tokens.fc1.weight', 'blocks.9.mlp_tokens.fc2.bias', 'blocks.9.mlp_tokens.fc2.weight', 'head.bias', 'head.weight', 'norm.bias', 'norm.weight', 'stem.proj.bias', 'stem.proj.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b0968f-7752-4ae2-9c09-66510cfc34d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['blocks.0.norm1.bias', 'blocks.0.norm1.weight', 'blocks.0.norm2.bias', 'blocks.0.norm2.weight', 'blocks.0.mlp_channels.fc1.bias', 'blocks.0.mlp_channels.fc1.weight', 'blocks.0.mlp_channels.fc2.bias', 'blocks.0.mlp_channels.fc2.weight', 'blocks.0.mlp_tokens.fc1.bias', 'blocks.0.mlp_tokens.fc1.weight', 'blocks.0.mlp_tokens.fc2.bias', 'blocks.0.mlp_tokens.fc2.weight', 'blocks.1.norm1.bias', 'blocks.1.norm1.weight', 'blocks.1.norm2.bias', 'blocks.1.norm2.weight', 'blocks.1.mlp_channels.fc1.bias', 'blocks.1.mlp_channels.fc1.weight', 'blocks.1.mlp_channels.fc2.bias', 'blocks.1.mlp_channels.fc2.weight', 'blocks.1.mlp_tokens.fc1.bias', 'blocks.1.mlp_tokens.fc1.weight', 'blocks.1.mlp_tokens.fc2.bias', 'blocks.1.mlp_tokens.fc2.weight', 'blocks.10.norm1.bias', 'blocks.10.norm1.weight', 'blocks.10.norm2.bias', 'blocks.10.norm2.weight', 'blocks.10.mlp_channels.fc1.bias', 'blocks.10.mlp_channels.fc1.weight', 'blocks.10.mlp_channels.fc2.bias', 'blocks.10.mlp_channels.fc2.weight', 'blocks.10.mlp_tokens.fc1.bias', 'blocks.10.mlp_tokens.fc1.weight', 'blocks.10.mlp_tokens.fc2.bias', 'blocks.10.mlp_tokens.fc2.weight', 'blocks.11.norm1.bias', 'blocks.11.norm1.weight', 'blocks.11.norm2.bias', 'blocks.11.norm2.weight', 'blocks.11.mlp_channels.fc1.bias', 'blocks.11.mlp_channels.fc1.weight', 'blocks.11.mlp_channels.fc2.bias', 'blocks.11.mlp_channels.fc2.weight', 'blocks.11.mlp_tokens.fc1.bias', 'blocks.11.mlp_tokens.fc1.weight', 'blocks.11.mlp_tokens.fc2.bias', 'blocks.11.mlp_tokens.fc2.weight', 'blocks.2.norm1.bias', 'blocks.2.norm1.weight', 'blocks.2.norm2.bias', 'blocks.2.norm2.weight', 'blocks.2.mlp_channels.fc1.bias', 'blocks.2.mlp_channels.fc1.weight', 'blocks.2.mlp_channels.fc2.bias', 'blocks.2.mlp_channels.fc2.weight', 'blocks.2.mlp_tokens.fc1.bias', 'blocks.2.mlp_tokens.fc1.weight', 'blocks.2.mlp_tokens.fc2.bias', 'blocks.2.mlp_tokens.fc2.weight', 'blocks.3.norm1.bias', 'blocks.3.norm1.weight', 'blocks.3.norm2.bias', 'blocks.3.norm2.weight', 'blocks.3.mlp_channels.fc1.bias', 'blocks.3.mlp_channels.fc1.weight', 'blocks.3.mlp_channels.fc2.bias', 'blocks.3.mlp_channels.fc2.weight', 'blocks.3.mlp_tokens.fc1.bias', 'blocks.3.mlp_tokens.fc1.weight', 'blocks.3.mlp_tokens.fc2.bias', 'blocks.3.mlp_tokens.fc2.weight', 'blocks.4.norm1.bias', 'blocks.4.norm1.weight', 'blocks.4.norm2.bias', 'blocks.4.norm2.weight', 'blocks.4.mlp_channels.fc1.bias', 'blocks.4.mlp_channels.fc1.weight', 'blocks.4.mlp_channels.fc2.bias', 'blocks.4.mlp_channels.fc2.weight', 'blocks.4.mlp_tokens.fc1.bias', 'blocks.4.mlp_tokens.fc1.weight', 'blocks.4.mlp_tokens.fc2.bias', 'blocks.4.mlp_tokens.fc2.weight', 'blocks.5.norm1.bias', 'blocks.5.norm1.weight', 'blocks.5.norm2.bias', 'blocks.5.norm2.weight', 'blocks.5.mlp_channels.fc1.bias', 'blocks.5.mlp_channels.fc1.weight', 'blocks.5.mlp_channels.fc2.bias', 'blocks.5.mlp_channels.fc2.weight', 'blocks.5.mlp_tokens.fc1.bias', 'blocks.5.mlp_tokens.fc1.weight', 'blocks.5.mlp_tokens.fc2.bias', 'blocks.5.mlp_tokens.fc2.weight', 'blocks.6.norm1.bias', 'blocks.6.norm1.weight', 'blocks.6.norm2.bias', 'blocks.6.norm2.weight', 'blocks.6.mlp_channels.fc1.bias', 'blocks.6.mlp_channels.fc1.weight', 'blocks.6.mlp_channels.fc2.bias', 'blocks.6.mlp_channels.fc2.weight', 'blocks.6.mlp_tokens.fc1.bias', 'blocks.6.mlp_tokens.fc1.weight', 'blocks.6.mlp_tokens.fc2.bias', 'blocks.6.mlp_tokens.fc2.weight', 'blocks.7.norm1.bias', 'blocks.7.norm1.weight', 'blocks.7.norm2.bias', 'blocks.7.norm2.weight', 'blocks.7.mlp_channels.fc1.bias', 'blocks.7.mlp_channels.fc1.weight', 'blocks.7.mlp_channels.fc2.bias', 'blocks.7.mlp_channels.fc2.weight', 'blocks.7.mlp_tokens.fc1.bias', 'blocks.7.mlp_tokens.fc1.weight', 'blocks.7.mlp_tokens.fc2.bias', 'blocks.7.mlp_tokens.fc2.weight', 'blocks.8.norm1.bias', 'blocks.8.norm1.weight', 'blocks.8.norm2.bias', 'blocks.8.norm2.weight', 'blocks.8.mlp_channels.fc1.bias', 'blocks.8.mlp_channels.fc1.weight', 'blocks.8.mlp_channels.fc2.bias', 'blocks.8.mlp_channels.fc2.weight', 'blocks.8.mlp_tokens.fc1.bias', 'blocks.8.mlp_tokens.fc1.weight', 'blocks.8.mlp_tokens.fc2.bias', 'blocks.8.mlp_tokens.fc2.weight', 'blocks.9.norm1.bias', 'blocks.9.norm1.weight', 'blocks.9.norm2.bias', 'blocks.9.norm2.weight', 'blocks.9.mlp_channels.fc1.bias', 'blocks.9.mlp_channels.fc1.weight', 'blocks.9.mlp_channels.fc2.bias', 'blocks.9.mlp_channels.fc2.weight', 'blocks.9.mlp_tokens.fc1.bias', 'blocks.9.mlp_tokens.fc1.weight', 'blocks.9.mlp_tokens.fc2.bias', 'blocks.9.mlp_tokens.fc2.weight', 'head.bias', 'head.weight', 'norm.bias', 'norm.weight', 'stem.proj.bias', 'stem.proj.weight'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92c52619-22d0-47b6-a240-f292d8e86215",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.block.0.norm1.weight\", \"Transformer.block.0.norm1.bias\", \"Transformer.block.0.MHSA.project.weight\", \"Transformer.block.0.MHSA.project.bias\", \"Transformer.block.0.norm2.weight\", \"Transformer.block.0.norm2.bias\", \"Transformer.block.0.mlp_channels.l1.weight\", \"Transformer.block.0.mlp_channels.l1.bias\", \"Transformer.block.0.mlp_channels.l2.weight\", \"Transformer.block.0.mlp_channels.l2.bias\", \"Transformer.block.1.norm1.weight\", \"Transformer.block.1.norm1.bias\", \"Transformer.block.1.MHSA.project.weight\", \"Transformer.block.1.MHSA.project.bias\", \"Transformer.block.1.norm2.weight\", \"Transformer.block.1.norm2.bias\", \"Transformer.block.1.mlp_channels.l1.weight\", \"Transformer.block.1.mlp_channels.l1.bias\", \"Transformer.block.1.mlp_channels.l2.weight\", \"Transformer.block.1.mlp_channels.l2.bias\", \"Transformer.block.2.norm1.weight\", \"Transformer.block.2.norm1.bias\", \"Transformer.block.2.MHSA.project.weight\", \"Transformer.block.2.MHSA.project.bias\", \"Transformer.block.2.norm2.weight\", \"Transformer.block.2.norm2.bias\", \"Transformer.block.2.mlp_channels.l1.weight\", \"Transformer.block.2.mlp_channels.l1.bias\", \"Transformer.block.2.mlp_channels.l2.weight\", \"Transformer.block.2.mlp_channels.l2.bias\", \"Transformer.block.3.norm1.weight\", \"Transformer.block.3.norm1.bias\", \"Transformer.block.3.MHSA.project.weight\", \"Transformer.block.3.MHSA.project.bias\", \"Transformer.block.3.norm2.weight\", \"Transformer.block.3.norm2.bias\", \"Transformer.block.3.mlp_channels.l1.weight\", \"Transformer.block.3.mlp_channels.l1.bias\", \"Transformer.block.3.mlp_channels.l2.weight\", \"Transformer.block.3.mlp_channels.l2.bias\", \"Transformer.block.4.norm1.weight\", \"Transformer.block.4.norm1.bias\", \"Transformer.block.4.MHSA.project.weight\", \"Transformer.block.4.MHSA.project.bias\", \"Transformer.block.4.norm2.weight\", \"Transformer.block.4.norm2.bias\", \"Transformer.block.4.mlp_channels.l1.weight\", \"Transformer.block.4.mlp_channels.l1.bias\", \"Transformer.block.4.mlp_channels.l2.weight\", \"Transformer.block.4.mlp_channels.l2.bias\", \"Transformer.block.5.norm1.weight\", \"Transformer.block.5.norm1.bias\", \"Transformer.block.5.MHSA.project.weight\", \"Transformer.block.5.MHSA.project.bias\", \"Transformer.block.5.norm2.weight\", \"Transformer.block.5.norm2.bias\", \"Transformer.block.5.mlp_channels.l1.weight\", \"Transformer.block.5.mlp_channels.l1.bias\", \"Transformer.block.5.mlp_channels.l2.weight\", \"Transformer.block.5.mlp_channels.l2.bias\", \"Transformer.block.6.norm1.weight\", \"Transformer.block.6.norm1.bias\", \"Transformer.block.6.MHSA.project.weight\", \"Transformer.block.6.MHSA.project.bias\", \"Transformer.block.6.norm2.weight\", \"Transformer.block.6.norm2.bias\", \"Transformer.block.6.mlp_channels.l1.weight\", \"Transformer.block.6.mlp_channels.l1.bias\", \"Transformer.block.6.mlp_channels.l2.weight\", \"Transformer.block.6.mlp_channels.l2.bias\", \"Transformer.block.7.norm1.weight\", \"Transformer.block.7.norm1.bias\", \"Transformer.block.7.MHSA.project.weight\", \"Transformer.block.7.MHSA.project.bias\", \"Transformer.block.7.norm2.weight\", \"Transformer.block.7.norm2.bias\", \"Transformer.block.7.mlp_channels.l1.weight\", \"Transformer.block.7.mlp_channels.l1.bias\", \"Transformer.block.7.mlp_channels.l2.weight\", \"Transformer.block.7.mlp_channels.l2.bias\", \"Transformer.block.8.norm1.weight\", \"Transformer.block.8.norm1.bias\", \"Transformer.block.8.MHSA.project.weight\", \"Transformer.block.8.MHSA.project.bias\", \"Transformer.block.8.norm2.weight\", \"Transformer.block.8.norm2.bias\", \"Transformer.block.8.mlp_channels.l1.weight\", \"Transformer.block.8.mlp_channels.l1.bias\", \"Transformer.block.8.mlp_channels.l2.weight\", \"Transformer.block.8.mlp_channels.l2.bias\", \"Transformer.block.9.norm1.weight\", \"Transformer.block.9.norm1.bias\", \"Transformer.block.9.MHSA.project.weight\", \"Transformer.block.9.MHSA.project.bias\", \"Transformer.block.9.norm2.weight\", \"Transformer.block.9.norm2.bias\", \"Transformer.block.9.mlp_channels.l1.weight\", \"Transformer.block.9.mlp_channels.l1.bias\", \"Transformer.block.9.mlp_channels.l2.weight\", \"Transformer.block.9.mlp_channels.l2.bias\", \"Transformer.block.10.norm1.weight\", \"Transformer.block.10.norm1.bias\", \"Transformer.block.10.MHSA.project.weight\", \"Transformer.block.10.MHSA.project.bias\", \"Transformer.block.10.norm2.weight\", \"Transformer.block.10.norm2.bias\", \"Transformer.block.10.mlp_channels.l1.weight\", \"Transformer.block.10.mlp_channels.l1.bias\", \"Transformer.block.10.mlp_channels.l2.weight\", \"Transformer.block.10.mlp_channels.l2.bias\", \"Transformer.block.11.norm1.weight\", \"Transformer.block.11.norm1.bias\", \"Transformer.block.11.MHSA.project.weight\", \"Transformer.block.11.MHSA.project.bias\", \"Transformer.block.11.norm2.weight\", \"Transformer.block.11.norm2.bias\", \"Transformer.block.11.mlp_channels.l1.weight\", \"Transformer.block.11.mlp_channels.l1.bias\", \"Transformer.block.11.mlp_channels.l2.weight\", \"Transformer.block.11.mlp_channels.l2.bias\", \"mlp_head.weight\", \"mlp_head.bias\". \n\tUnexpected key(s) in state_dict: \"blocks.0.norm1.bias\", \"blocks.0.norm1.weight\", \"blocks.0.norm2.bias\", \"blocks.0.norm2.weight\", \"blocks.0.mlp_channels.fc1.bias\", \"blocks.0.mlp_channels.fc1.weight\", \"blocks.0.mlp_channels.fc2.bias\", \"blocks.0.mlp_channels.fc2.weight\", \"blocks.0.mlp_tokens.fc1.bias\", \"blocks.0.mlp_tokens.fc1.weight\", \"blocks.0.mlp_tokens.fc2.bias\", \"blocks.0.mlp_tokens.fc2.weight\", \"blocks.1.norm1.bias\", \"blocks.1.norm1.weight\", \"blocks.1.norm2.bias\", \"blocks.1.norm2.weight\", \"blocks.1.mlp_channels.fc1.bias\", \"blocks.1.mlp_channels.fc1.weight\", \"blocks.1.mlp_channels.fc2.bias\", \"blocks.1.mlp_channels.fc2.weight\", \"blocks.1.mlp_tokens.fc1.bias\", \"blocks.1.mlp_tokens.fc1.weight\", \"blocks.1.mlp_tokens.fc2.bias\", \"blocks.1.mlp_tokens.fc2.weight\", \"blocks.10.norm1.bias\", \"blocks.10.norm1.weight\", \"blocks.10.norm2.bias\", \"blocks.10.norm2.weight\", \"blocks.10.mlp_channels.fc1.bias\", \"blocks.10.mlp_channels.fc1.weight\", \"blocks.10.mlp_channels.fc2.bias\", \"blocks.10.mlp_channels.fc2.weight\", \"blocks.10.mlp_tokens.fc1.bias\", \"blocks.10.mlp_tokens.fc1.weight\", \"blocks.10.mlp_tokens.fc2.bias\", \"blocks.10.mlp_tokens.fc2.weight\", \"blocks.11.norm1.bias\", \"blocks.11.norm1.weight\", \"blocks.11.norm2.bias\", \"blocks.11.norm2.weight\", \"blocks.11.mlp_channels.fc1.bias\", \"blocks.11.mlp_channels.fc1.weight\", \"blocks.11.mlp_channels.fc2.bias\", \"blocks.11.mlp_channels.fc2.weight\", \"blocks.11.mlp_tokens.fc1.bias\", \"blocks.11.mlp_tokens.fc1.weight\", \"blocks.11.mlp_tokens.fc2.bias\", \"blocks.11.mlp_tokens.fc2.weight\", \"blocks.2.norm1.bias\", \"blocks.2.norm1.weight\", \"blocks.2.norm2.bias\", \"blocks.2.norm2.weight\", \"blocks.2.mlp_channels.fc1.bias\", \"blocks.2.mlp_channels.fc1.weight\", \"blocks.2.mlp_channels.fc2.bias\", \"blocks.2.mlp_channels.fc2.weight\", \"blocks.2.mlp_tokens.fc1.bias\", \"blocks.2.mlp_tokens.fc1.weight\", \"blocks.2.mlp_tokens.fc2.bias\", \"blocks.2.mlp_tokens.fc2.weight\", \"blocks.3.norm1.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm2.bias\", \"blocks.3.norm2.weight\", \"blocks.3.mlp_channels.fc1.bias\", \"blocks.3.mlp_channels.fc1.weight\", \"blocks.3.mlp_channels.fc2.bias\", \"blocks.3.mlp_channels.fc2.weight\", \"blocks.3.mlp_tokens.fc1.bias\", \"blocks.3.mlp_tokens.fc1.weight\", \"blocks.3.mlp_tokens.fc2.bias\", \"blocks.3.mlp_tokens.fc2.weight\", \"blocks.4.norm1.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm2.bias\", \"blocks.4.norm2.weight\", \"blocks.4.mlp_channels.fc1.bias\", \"blocks.4.mlp_channels.fc1.weight\", \"blocks.4.mlp_channels.fc2.bias\", \"blocks.4.mlp_channels.fc2.weight\", \"blocks.4.mlp_tokens.fc1.bias\", \"blocks.4.mlp_tokens.fc1.weight\", \"blocks.4.mlp_tokens.fc2.bias\", \"blocks.4.mlp_tokens.fc2.weight\", \"blocks.5.norm1.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm2.bias\", \"blocks.5.norm2.weight\", \"blocks.5.mlp_channels.fc1.bias\", \"blocks.5.mlp_channels.fc1.weight\", \"blocks.5.mlp_channels.fc2.bias\", \"blocks.5.mlp_channels.fc2.weight\", \"blocks.5.mlp_tokens.fc1.bias\", \"blocks.5.mlp_tokens.fc1.weight\", \"blocks.5.mlp_tokens.fc2.bias\", \"blocks.5.mlp_tokens.fc2.weight\", \"blocks.6.norm1.bias\", \"blocks.6.norm1.weight\", \"blocks.6.norm2.bias\", \"blocks.6.norm2.weight\", \"blocks.6.mlp_channels.fc1.bias\", \"blocks.6.mlp_channels.fc1.weight\", \"blocks.6.mlp_channels.fc2.bias\", \"blocks.6.mlp_channels.fc2.weight\", \"blocks.6.mlp_tokens.fc1.bias\", \"blocks.6.mlp_tokens.fc1.weight\", \"blocks.6.mlp_tokens.fc2.bias\", \"blocks.6.mlp_tokens.fc2.weight\", \"blocks.7.norm1.bias\", \"blocks.7.norm1.weight\", \"blocks.7.norm2.bias\", \"blocks.7.norm2.weight\", \"blocks.7.mlp_channels.fc1.bias\", \"blocks.7.mlp_channels.fc1.weight\", \"blocks.7.mlp_channels.fc2.bias\", \"blocks.7.mlp_channels.fc2.weight\", \"blocks.7.mlp_tokens.fc1.bias\", \"blocks.7.mlp_tokens.fc1.weight\", \"blocks.7.mlp_tokens.fc2.bias\", \"blocks.7.mlp_tokens.fc2.weight\", \"blocks.8.norm1.bias\", \"blocks.8.norm1.weight\", \"blocks.8.norm2.bias\", \"blocks.8.norm2.weight\", \"blocks.8.mlp_channels.fc1.bias\", \"blocks.8.mlp_channels.fc1.weight\", \"blocks.8.mlp_channels.fc2.bias\", \"blocks.8.mlp_channels.fc2.weight\", \"blocks.8.mlp_tokens.fc1.bias\", \"blocks.8.mlp_tokens.fc1.weight\", \"blocks.8.mlp_tokens.fc2.bias\", \"blocks.8.mlp_tokens.fc2.weight\", \"blocks.9.norm1.bias\", \"blocks.9.norm1.weight\", \"blocks.9.norm2.bias\", \"blocks.9.norm2.weight\", \"blocks.9.mlp_channels.fc1.bias\", \"blocks.9.mlp_channels.fc1.weight\", \"blocks.9.mlp_channels.fc2.bias\", \"blocks.9.mlp_channels.fc2.weight\", \"blocks.9.mlp_tokens.fc1.bias\", \"blocks.9.mlp_tokens.fc1.weight\", \"blocks.9.mlp_tokens.fc2.bias\", \"blocks.9.mlp_tokens.fc2.weight\", \"head.bias\", \"head.weight\", \"stem.proj.bias\", \"stem.proj.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# i literally have to rename everything in my model to match these weights KMS\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\peep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.block.0.norm1.weight\", \"Transformer.block.0.norm1.bias\", \"Transformer.block.0.MHSA.project.weight\", \"Transformer.block.0.MHSA.project.bias\", \"Transformer.block.0.norm2.weight\", \"Transformer.block.0.norm2.bias\", \"Transformer.block.0.mlp_channels.l1.weight\", \"Transformer.block.0.mlp_channels.l1.bias\", \"Transformer.block.0.mlp_channels.l2.weight\", \"Transformer.block.0.mlp_channels.l2.bias\", \"Transformer.block.1.norm1.weight\", \"Transformer.block.1.norm1.bias\", \"Transformer.block.1.MHSA.project.weight\", \"Transformer.block.1.MHSA.project.bias\", \"Transformer.block.1.norm2.weight\", \"Transformer.block.1.norm2.bias\", \"Transformer.block.1.mlp_channels.l1.weight\", \"Transformer.block.1.mlp_channels.l1.bias\", \"Transformer.block.1.mlp_channels.l2.weight\", \"Transformer.block.1.mlp_channels.l2.bias\", \"Transformer.block.2.norm1.weight\", \"Transformer.block.2.norm1.bias\", \"Transformer.block.2.MHSA.project.weight\", \"Transformer.block.2.MHSA.project.bias\", \"Transformer.block.2.norm2.weight\", \"Transformer.block.2.norm2.bias\", \"Transformer.block.2.mlp_channels.l1.weight\", \"Transformer.block.2.mlp_channels.l1.bias\", \"Transformer.block.2.mlp_channels.l2.weight\", \"Transformer.block.2.mlp_channels.l2.bias\", \"Transformer.block.3.norm1.weight\", \"Transformer.block.3.norm1.bias\", \"Transformer.block.3.MHSA.project.weight\", \"Transformer.block.3.MHSA.project.bias\", \"Transformer.block.3.norm2.weight\", \"Transformer.block.3.norm2.bias\", \"Transformer.block.3.mlp_channels.l1.weight\", \"Transformer.block.3.mlp_channels.l1.bias\", \"Transformer.block.3.mlp_channels.l2.weight\", \"Transformer.block.3.mlp_channels.l2.bias\", \"Transformer.block.4.norm1.weight\", \"Transformer.block.4.norm1.bias\", \"Transformer.block.4.MHSA.project.weight\", \"Transformer.block.4.MHSA.project.bias\", \"Transformer.block.4.norm2.weight\", \"Transformer.block.4.norm2.bias\", \"Transformer.block.4.mlp_channels.l1.weight\", \"Transformer.block.4.mlp_channels.l1.bias\", \"Transformer.block.4.mlp_channels.l2.weight\", \"Transformer.block.4.mlp_channels.l2.bias\", \"Transformer.block.5.norm1.weight\", \"Transformer.block.5.norm1.bias\", \"Transformer.block.5.MHSA.project.weight\", \"Transformer.block.5.MHSA.project.bias\", \"Transformer.block.5.norm2.weight\", \"Transformer.block.5.norm2.bias\", \"Transformer.block.5.mlp_channels.l1.weight\", \"Transformer.block.5.mlp_channels.l1.bias\", \"Transformer.block.5.mlp_channels.l2.weight\", \"Transformer.block.5.mlp_channels.l2.bias\", \"Transformer.block.6.norm1.weight\", \"Transformer.block.6.norm1.bias\", \"Transformer.block.6.MHSA.project.weight\", \"Transformer.block.6.MHSA.project.bias\", \"Transformer.block.6.norm2.weight\", \"Transformer.block.6.norm2.bias\", \"Transformer.block.6.mlp_channels.l1.weight\", \"Transformer.block.6.mlp_channels.l1.bias\", \"Transformer.block.6.mlp_channels.l2.weight\", \"Transformer.block.6.mlp_channels.l2.bias\", \"Transformer.block.7.norm1.weight\", \"Transformer.block.7.norm1.bias\", \"Transformer.block.7.MHSA.project.weight\", \"Transformer.block.7.MHSA.project.bias\", \"Transformer.block.7.norm2.weight\", \"Transformer.block.7.norm2.bias\", \"Transformer.block.7.mlp_channels.l1.weight\", \"Transformer.block.7.mlp_channels.l1.bias\", \"Transformer.block.7.mlp_channels.l2.weight\", \"Transformer.block.7.mlp_channels.l2.bias\", \"Transformer.block.8.norm1.weight\", \"Transformer.block.8.norm1.bias\", \"Transformer.block.8.MHSA.project.weight\", \"Transformer.block.8.MHSA.project.bias\", \"Transformer.block.8.norm2.weight\", \"Transformer.block.8.norm2.bias\", \"Transformer.block.8.mlp_channels.l1.weight\", \"Transformer.block.8.mlp_channels.l1.bias\", \"Transformer.block.8.mlp_channels.l2.weight\", \"Transformer.block.8.mlp_channels.l2.bias\", \"Transformer.block.9.norm1.weight\", \"Transformer.block.9.norm1.bias\", \"Transformer.block.9.MHSA.project.weight\", \"Transformer.block.9.MHSA.project.bias\", \"Transformer.block.9.norm2.weight\", \"Transformer.block.9.norm2.bias\", \"Transformer.block.9.mlp_channels.l1.weight\", \"Transformer.block.9.mlp_channels.l1.bias\", \"Transformer.block.9.mlp_channels.l2.weight\", \"Transformer.block.9.mlp_channels.l2.bias\", \"Transformer.block.10.norm1.weight\", \"Transformer.block.10.norm1.bias\", \"Transformer.block.10.MHSA.project.weight\", \"Transformer.block.10.MHSA.project.bias\", \"Transformer.block.10.norm2.weight\", \"Transformer.block.10.norm2.bias\", \"Transformer.block.10.mlp_channels.l1.weight\", \"Transformer.block.10.mlp_channels.l1.bias\", \"Transformer.block.10.mlp_channels.l2.weight\", \"Transformer.block.10.mlp_channels.l2.bias\", \"Transformer.block.11.norm1.weight\", \"Transformer.block.11.norm1.bias\", \"Transformer.block.11.MHSA.project.weight\", \"Transformer.block.11.MHSA.project.bias\", \"Transformer.block.11.norm2.weight\", \"Transformer.block.11.norm2.bias\", \"Transformer.block.11.mlp_channels.l1.weight\", \"Transformer.block.11.mlp_channels.l1.bias\", \"Transformer.block.11.mlp_channels.l2.weight\", \"Transformer.block.11.mlp_channels.l2.bias\", \"mlp_head.weight\", \"mlp_head.bias\". \n\tUnexpected key(s) in state_dict: \"blocks.0.norm1.bias\", \"blocks.0.norm1.weight\", \"blocks.0.norm2.bias\", \"blocks.0.norm2.weight\", \"blocks.0.mlp_channels.fc1.bias\", \"blocks.0.mlp_channels.fc1.weight\", \"blocks.0.mlp_channels.fc2.bias\", \"blocks.0.mlp_channels.fc2.weight\", \"blocks.0.mlp_tokens.fc1.bias\", \"blocks.0.mlp_tokens.fc1.weight\", \"blocks.0.mlp_tokens.fc2.bias\", \"blocks.0.mlp_tokens.fc2.weight\", \"blocks.1.norm1.bias\", \"blocks.1.norm1.weight\", \"blocks.1.norm2.bias\", \"blocks.1.norm2.weight\", \"blocks.1.mlp_channels.fc1.bias\", \"blocks.1.mlp_channels.fc1.weight\", \"blocks.1.mlp_channels.fc2.bias\", \"blocks.1.mlp_channels.fc2.weight\", \"blocks.1.mlp_tokens.fc1.bias\", \"blocks.1.mlp_tokens.fc1.weight\", \"blocks.1.mlp_tokens.fc2.bias\", \"blocks.1.mlp_tokens.fc2.weight\", \"blocks.10.norm1.bias\", \"blocks.10.norm1.weight\", \"blocks.10.norm2.bias\", \"blocks.10.norm2.weight\", \"blocks.10.mlp_channels.fc1.bias\", \"blocks.10.mlp_channels.fc1.weight\", \"blocks.10.mlp_channels.fc2.bias\", \"blocks.10.mlp_channels.fc2.weight\", \"blocks.10.mlp_tokens.fc1.bias\", \"blocks.10.mlp_tokens.fc1.weight\", \"blocks.10.mlp_tokens.fc2.bias\", \"blocks.10.mlp_tokens.fc2.weight\", \"blocks.11.norm1.bias\", \"blocks.11.norm1.weight\", \"blocks.11.norm2.bias\", \"blocks.11.norm2.weight\", \"blocks.11.mlp_channels.fc1.bias\", \"blocks.11.mlp_channels.fc1.weight\", \"blocks.11.mlp_channels.fc2.bias\", \"blocks.11.mlp_channels.fc2.weight\", \"blocks.11.mlp_tokens.fc1.bias\", \"blocks.11.mlp_tokens.fc1.weight\", \"blocks.11.mlp_tokens.fc2.bias\", \"blocks.11.mlp_tokens.fc2.weight\", \"blocks.2.norm1.bias\", \"blocks.2.norm1.weight\", \"blocks.2.norm2.bias\", \"blocks.2.norm2.weight\", \"blocks.2.mlp_channels.fc1.bias\", \"blocks.2.mlp_channels.fc1.weight\", \"blocks.2.mlp_channels.fc2.bias\", \"blocks.2.mlp_channels.fc2.weight\", \"blocks.2.mlp_tokens.fc1.bias\", \"blocks.2.mlp_tokens.fc1.weight\", \"blocks.2.mlp_tokens.fc2.bias\", \"blocks.2.mlp_tokens.fc2.weight\", \"blocks.3.norm1.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm2.bias\", \"blocks.3.norm2.weight\", \"blocks.3.mlp_channels.fc1.bias\", \"blocks.3.mlp_channels.fc1.weight\", \"blocks.3.mlp_channels.fc2.bias\", \"blocks.3.mlp_channels.fc2.weight\", \"blocks.3.mlp_tokens.fc1.bias\", \"blocks.3.mlp_tokens.fc1.weight\", \"blocks.3.mlp_tokens.fc2.bias\", \"blocks.3.mlp_tokens.fc2.weight\", \"blocks.4.norm1.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm2.bias\", \"blocks.4.norm2.weight\", \"blocks.4.mlp_channels.fc1.bias\", \"blocks.4.mlp_channels.fc1.weight\", \"blocks.4.mlp_channels.fc2.bias\", \"blocks.4.mlp_channels.fc2.weight\", \"blocks.4.mlp_tokens.fc1.bias\", \"blocks.4.mlp_tokens.fc1.weight\", \"blocks.4.mlp_tokens.fc2.bias\", \"blocks.4.mlp_tokens.fc2.weight\", \"blocks.5.norm1.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm2.bias\", \"blocks.5.norm2.weight\", \"blocks.5.mlp_channels.fc1.bias\", \"blocks.5.mlp_channels.fc1.weight\", \"blocks.5.mlp_channels.fc2.bias\", \"blocks.5.mlp_channels.fc2.weight\", \"blocks.5.mlp_tokens.fc1.bias\", \"blocks.5.mlp_tokens.fc1.weight\", \"blocks.5.mlp_tokens.fc2.bias\", \"blocks.5.mlp_tokens.fc2.weight\", \"blocks.6.norm1.bias\", \"blocks.6.norm1.weight\", \"blocks.6.norm2.bias\", \"blocks.6.norm2.weight\", \"blocks.6.mlp_channels.fc1.bias\", \"blocks.6.mlp_channels.fc1.weight\", \"blocks.6.mlp_channels.fc2.bias\", \"blocks.6.mlp_channels.fc2.weight\", \"blocks.6.mlp_tokens.fc1.bias\", \"blocks.6.mlp_tokens.fc1.weight\", \"blocks.6.mlp_tokens.fc2.bias\", \"blocks.6.mlp_tokens.fc2.weight\", \"blocks.7.norm1.bias\", \"blocks.7.norm1.weight\", \"blocks.7.norm2.bias\", \"blocks.7.norm2.weight\", \"blocks.7.mlp_channels.fc1.bias\", \"blocks.7.mlp_channels.fc1.weight\", \"blocks.7.mlp_channels.fc2.bias\", \"blocks.7.mlp_channels.fc2.weight\", \"blocks.7.mlp_tokens.fc1.bias\", \"blocks.7.mlp_tokens.fc1.weight\", \"blocks.7.mlp_tokens.fc2.bias\", \"blocks.7.mlp_tokens.fc2.weight\", \"blocks.8.norm1.bias\", \"blocks.8.norm1.weight\", \"blocks.8.norm2.bias\", \"blocks.8.norm2.weight\", \"blocks.8.mlp_channels.fc1.bias\", \"blocks.8.mlp_channels.fc1.weight\", \"blocks.8.mlp_channels.fc2.bias\", \"blocks.8.mlp_channels.fc2.weight\", \"blocks.8.mlp_tokens.fc1.bias\", \"blocks.8.mlp_tokens.fc1.weight\", \"blocks.8.mlp_tokens.fc2.bias\", \"blocks.8.mlp_tokens.fc2.weight\", \"blocks.9.norm1.bias\", \"blocks.9.norm1.weight\", \"blocks.9.norm2.bias\", \"blocks.9.norm2.weight\", \"blocks.9.mlp_channels.fc1.bias\", \"blocks.9.mlp_channels.fc1.weight\", \"blocks.9.mlp_channels.fc2.bias\", \"blocks.9.mlp_channels.fc2.weight\", \"blocks.9.mlp_tokens.fc1.bias\", \"blocks.9.mlp_tokens.fc1.weight\", \"blocks.9.mlp_tokens.fc2.bias\", \"blocks.9.mlp_tokens.fc2.weight\", \"head.bias\", \"head.weight\", \"stem.proj.bias\", \"stem.proj.weight\". "
     ]
    }
   ],
   "source": [
    "# i literally have to rename everything in my model to match these weights KMS\n",
    "model.load_state_dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66e93a2b-839e-4e23-86f9-161610036868",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned = \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.block.0.norm1.weight\", \"Transformer.block.0.norm1.bias\", \"Transformer.block.0.MHSA.project.weight\", \"Transformer.block.0.MHSA.project.bias\", \"Transformer.block.0.norm2.weight\", \"Transformer.block.0.norm2.bias\", \"Transformer.block.0.MLP.l1.weight\", \"Transformer.block.0.MLP.l1.bias\", \"Transformer.block.0.MLP.l2.weight\", \"Transformer.block.0.MLP.l2.bias\", \"Transformer.block.1.norm1.weight\", \"Transformer.block.1.norm1.bias\", \"Transformer.block.1.MHSA.project.weight\", \"Transformer.block.1.MHSA.project.bias\", \"Transformer.block.1.norm2.weight\", \"Transformer.block.1.norm2.bias\", \"Transformer.block.1.MLP.l1.weight\", \"Transformer.block.1.MLP.l1.bias\", \"Transformer.block.1.MLP.l2.weight\", \"Transformer.block.1.MLP.l2.bias\", \"Transformer.block.2.norm1.weight\", \"Transformer.block.2.norm1.bias\", \"Transformer.block.2.MHSA.project.weight\", \"Transformer.block.2.MHSA.project.bias\", \"Transformer.block.2.norm2.weight\", \"Transformer.block.2.norm2.bias\", \"Transformer.block.2.MLP.l1.weight\", \"Transformer.block.2.MLP.l1.bias\", \"Transformer.block.2.MLP.l2.weight\", \"Transformer.block.2.MLP.l2.bias\", \"Transformer.block.3.norm1.weight\", \"Transformer.block.3.norm1.bias\", \"Transformer.block.3.MHSA.project.weight\", \"Transformer.block.3.MHSA.project.bias\", \"Transformer.block.3.norm2.weight\", \"Transformer.block.3.norm2.bias\", \"Transformer.block.3.MLP.l1.weight\", \"Transformer.block.3.MLP.l1.bias\", \"Transformer.block.3.MLP.l2.weight\", \"Transformer.block.3.MLP.l2.bias\", \"Transformer.block.4.norm1.weight\", \"Transformer.block.4.norm1.bias\", \"Transformer.block.4.MHSA.project.weight\", \"Transformer.block.4.MHSA.project.bias\", \"Transformer.block.4.norm2.weight\", \"Transformer.block.4.norm2.bias\", \"Transformer.block.4.MLP.l1.weight\", \"Transformer.block.4.MLP.l1.bias\", \"Transformer.block.4.MLP.l2.weight\", \"Transformer.block.4.MLP.l2.bias\", \"Transformer.block.5.norm1.weight\", \"Transformer.block.5.norm1.bias\", \"Transformer.block.5.MHSA.project.weight\", \"Transformer.block.5.MHSA.project.bias\", \"Transformer.block.5.norm2.weight\", \"Transformer.block.5.norm2.bias\", \"Transformer.block.5.MLP.l1.weight\", \"Transformer.block.5.MLP.l1.bias\", \"Transformer.block.5.MLP.l2.weight\", \"Transformer.block.5.MLP.l2.bias\", \"Transformer.block.6.norm1.weight\", \"Transformer.block.6.norm1.bias\", \"Transformer.block.6.MHSA.project.weight\", \"Transformer.block.6.MHSA.project.bias\", \"Transformer.block.6.norm2.weight\", \"Transformer.block.6.norm2.bias\", \"Transformer.block.6.MLP.l1.weight\", \"Transformer.block.6.MLP.l1.bias\", \"Transformer.block.6.MLP.l2.weight\", \"Transformer.block.6.MLP.l2.bias\", \"Transformer.block.7.norm1.weight\", \"Transformer.block.7.norm1.bias\", \"Transformer.block.7.MHSA.project.weight\", \"Transformer.block.7.MHSA.project.bias\", \"Transformer.block.7.norm2.weight\", \"Transformer.block.7.norm2.bias\", \"Transformer.block.7.MLP.l1.weight\", \"Transformer.block.7.MLP.l1.bias\", \"Transformer.block.7.MLP.l2.weight\", \"Transformer.block.7.MLP.l2.bias\", \"Transformer.block.8.norm1.weight\", \"Transformer.block.8.norm1.bias\", \"Transformer.block.8.MHSA.project.weight\", \"Transformer.block.8.MHSA.project.bias\", \"Transformer.block.8.norm2.weight\", \"Transformer.block.8.norm2.bias\", \"Transformer.block.8.MLP.l1.weight\", \"Transformer.block.8.MLP.l1.bias\", \"Transformer.block.8.MLP.l2.weight\", \"Transformer.block.8.MLP.l2.bias\", \"Transformer.block.9.norm1.weight\", \"Transformer.block.9.norm1.bias\", \"Transformer.block.9.MHSA.project.weight\", \"Transformer.block.9.MHSA.project.bias\", \"Transformer.block.9.norm2.weight\", \"Transformer.block.9.norm2.bias\", \"Transformer.block.9.MLP.l1.weight\", \"Transformer.block.9.MLP.l1.bias\", \"Transformer.block.9.MLP.l2.weight\", \"Transformer.block.9.MLP.l2.bias\", \"Transformer.block.10.norm1.weight\", \"Transformer.block.10.norm1.bias\", \"Transformer.block.10.MHSA.project.weight\", \"Transformer.block.10.MHSA.project.bias\", \"Transformer.block.10.norm2.weight\", \"Transformer.block.10.norm2.bias\", \"Transformer.block.10.MLP.l1.weight\", \"Transformer.block.10.MLP.l1.bias\", \"Transformer.block.10.MLP.l2.weight\", \"Transformer.block.10.MLP.l2.bias\", \"Transformer.block.11.norm1.weight\", \"Transformer.block.11.norm1.bias\", \"Transformer.block.11.MHSA.project.weight\", \"Transformer.block.11.MHSA.project.bias\", \"Transformer.block.11.norm2.weight\", \"Transformer.block.11.norm2.bias\", \"Transformer.block.11.MLP.l1.weight\", \"Transformer.block.11.MLP.l1.bias\", \"Transformer.block.11.MLP.l2.weight\", \"Transformer.block.11.MLP.l2.bias\", \"mlp_head.weight\", \"mlp_head.bias\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20f1ed6b-bdbe-483e-9c66-fd7972b9d9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ffb3963-3002-4ce3-83a7-3a3fcc731b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.block.0.norm1.weight\", \"Transformer.block.0.norm1.bias\", \"Transformer.block.0.MHSA.project.weight\", \"Transformer.block.0.MHSA.project.bias\", \"Transformer.block.0.norm2.weight\", \"Transformer.block.0.norm2.bias\", \"Transformer.block.0.mlp_channels.l1.weight\", \"Transformer.block.0.mlp_channels.l1.bias\", \"Transformer.block.0.mlp_channels.l2.weight\", \"Transformer.block.0.mlp_channels.l2.bias\", \"Transformer.block.1.norm1.weight\", \"Transformer.block.1.norm1.bias\", \"Transformer.block.1.MHSA.project.weight\", \"Transformer.block.1.MHSA.project.bias\", \"Transformer.block.1.norm2.weight\", \"Transformer.block.1.norm2.bias\", \"Transformer.block.1.mlp_channels.l1.weight\", \"Transformer.block.1.mlp_channels.l1.bias\", \"Transformer.block.1.mlp_channels.l2.weight\", \"Transformer.block.1.mlp_channels.l2.bias\", \"Transformer.block.2.norm1.weight\", \"Transformer.block.2.norm1.bias\", \"Transformer.block.2.MHSA.project.weight\", \"Transformer.block.2.MHSA.project.bias\", \"Transformer.block.2.norm2.weight\", \"Transformer.block.2.norm2.bias\", \"Transformer.block.2.mlp_channels.l1.weight\", \"Transformer.block.2.mlp_channels.l1.bias\", \"Transformer.block.2.mlp_channels.l2.weight\", \"Transformer.block.2.mlp_channels.l2.bias\", \"Transformer.block.3.norm1.weight\", \"Transformer.block.3.norm1.bias\", \"Transformer.block.3.MHSA.project.weight\", \"Transformer.block.3.MHSA.project.bias\", \"Transformer.block.3.norm2.weight\", \"Transformer.block.3.norm2.bias\", \"Transformer.block.3.mlp_channels.l1.weight\", \"Transformer.block.3.mlp_channels.l1.bias\", \"Transformer.block.3.mlp_channels.l2.weight\", \"Transformer.block.3.mlp_channels.l2.bias\", \"Transformer.block.4.norm1.weight\", \"Transformer.block.4.norm1.bias\", \"Transformer.block.4.MHSA.project.weight\", \"Transformer.block.4.MHSA.project.bias\", \"Transformer.block.4.norm2.weight\", \"Transformer.block.4.norm2.bias\", \"Transformer.block.4.mlp_channels.l1.weight\", \"Transformer.block.4.mlp_channels.l1.bias\", \"Transformer.block.4.mlp_channels.l2.weight\", \"Transformer.block.4.mlp_channels.l2.bias\", \"Transformer.block.5.norm1.weight\", \"Transformer.block.5.norm1.bias\", \"Transformer.block.5.MHSA.project.weight\", \"Transformer.block.5.MHSA.project.bias\", \"Transformer.block.5.norm2.weight\", \"Transformer.block.5.norm2.bias\", \"Transformer.block.5.mlp_channels.l1.weight\", \"Transformer.block.5.mlp_channels.l1.bias\", \"Transformer.block.5.mlp_channels.l2.weight\", \"Transformer.block.5.mlp_channels.l2.bias\", \"Transformer.block.6.norm1.weight\", \"Transformer.block.6.norm1.bias\", \"Transformer.block.6.MHSA.project.weight\", \"Transformer.block.6.MHSA.project.bias\", \"Transformer.block.6.norm2.weight\", \"Transformer.block.6.norm2.bias\", \"Transformer.block.6.mlp_channels.l1.weight\", \"Transformer.block.6.mlp_channels.l1.bias\", \"Transformer.block.6.mlp_channels.l2.weight\", \"Transformer.block.6.mlp_channels.l2.bias\", \"Transformer.block.7.norm1.weight\", \"Transformer.block.7.norm1.bias\", \"Transformer.block.7.MHSA.project.weight\", \"Transformer.block.7.MHSA.project.bias\", \"Transformer.block.7.norm2.weight\", \"Transformer.block.7.norm2.bias\", \"Transformer.block.7.mlp_channels.l1.weight\", \"Transformer.block.7.mlp_channels.l1.bias\", \"Transformer.block.7.mlp_channels.l2.weight\", \"Transformer.block.7.mlp_channels.l2.bias\", \"Transformer.block.8.norm1.weight\", \"Transformer.block.8.norm1.bias\", \"Transformer.block.8.MHSA.project.weight\", \"Transformer.block.8.MHSA.project.bias\", \"Transformer.block.8.norm2.weight\", \"Transformer.block.8.norm2.bias\", \"Transformer.block.8.mlp_channels.l1.weight\", \"Transformer.block.8.mlp_channels.l1.bias\", \"Transformer.block.8.mlp_channels.l2.weight\", \"Transformer.block.8.mlp_channels.l2.bias\", \"Transformer.block.9.norm1.weight\", \"Transformer.block.9.norm1.bias\", \"Transformer.block.9.MHSA.project.weight\", \"Transformer.block.9.MHSA.project.bias\", \"Transformer.block.9.norm2.weight\", \"Transformer.block.9.norm2.bias\", \"Transformer.block.9.mlp_channels.l1.weight\", \"Transformer.block.9.mlp_channels.l1.bias\", \"Transformer.block.9.mlp_channels.l2.weight\", \"Transformer.block.9.mlp_channels.l2.bias\", \"Transformer.block.10.norm1.weight\", \"Transformer.block.10.norm1.bias\", \"Transformer.block.10.MHSA.project.weight\", \"Transformer.block.10.MHSA.project.bias\", \"Transformer.block.10.norm2.weight\", \"Transformer.block.10.norm2.bias\", \"Transformer.block.10.mlp_channels.l1.weight\", \"Transformer.block.10.mlp_channels.l1.bias\", \"Transformer.block.10.mlp_channels.l2.weight\", \"Transformer.block.10.mlp_channels.l2.bias\", \"Transformer.block.11.norm1.weight\", \"Transformer.block.11.norm1.bias\", \"Transformer.block.11.MHSA.project.weight\", \"Transformer.block.11.MHSA.project.bias\", \"Transformer.block.11.norm2.weight\", \"Transformer.block.11.norm2.bias\", \"Transformer.block.11.mlp_channels.l1.weight\", \"Transformer.block.11.mlp_channels.l1.bias\", \"Transformer.block.11.mlp_channels.l2.weight\", \"Transformer.block.11.mlp_channels.l2.bias\", \"mlp_head.weight\", \"mlp_head.bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c6d139a-91ea-404b-8c7c-4f0df30af81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "af536bf2-1b4e-441f-aa35-fe6c8a6a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"b16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5d70adf-9ff0-4130-8ee3-b046dfa405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9cbf7da-1d61-4a22-92c1-c188b97bc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c8a69169-0040-4793-a55a-22ebc52560e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in x.items():\n",
    "    if i[:7] == \"blocks.\":\n",
    "        newkey = \"Transformer.\" + i\n",
    "        d[newkey] = j\n",
    "    else:\n",
    "        d[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51b4a642-2016-4773-9cfb-a1fd69d67c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Transformer.blocks.', 'head.bias', 'head.weight', 'norm.bias', 'norm.weight', 'stem.proj.bias', 'stem.proj.weight', 'Transformer.blocks.0.norm1.bias', 'Transformer.blocks.0.norm1.weight', 'Transformer.blocks.0.norm2.bias', 'Transformer.blocks.0.norm2.weight', 'Transformer.blocks.0.mlp_channels.fc1.bias', 'Transformer.blocks.0.mlp_channels.fc1.weight', 'Transformer.blocks.0.mlp_channels.fc2.bias', 'Transformer.blocks.0.mlp_channels.fc2.weight', 'Transformer.blocks.0.mlp_tokens.fc1.bias', 'Transformer.blocks.0.mlp_tokens.fc1.weight', 'Transformer.blocks.0.mlp_tokens.fc2.bias', 'Transformer.blocks.0.mlp_tokens.fc2.weight', 'Transformer.blocks.1.norm1.bias', 'Transformer.blocks.1.norm1.weight', 'Transformer.blocks.1.norm2.bias', 'Transformer.blocks.1.norm2.weight', 'Transformer.blocks.1.mlp_channels.fc1.bias', 'Transformer.blocks.1.mlp_channels.fc1.weight', 'Transformer.blocks.1.mlp_channels.fc2.bias', 'Transformer.blocks.1.mlp_channels.fc2.weight', 'Transformer.blocks.1.mlp_tokens.fc1.bias', 'Transformer.blocks.1.mlp_tokens.fc1.weight', 'Transformer.blocks.1.mlp_tokens.fc2.bias', 'Transformer.blocks.1.mlp_tokens.fc2.weight', 'Transformer.blocks.10.norm1.bias', 'Transformer.blocks.10.norm1.weight', 'Transformer.blocks.10.norm2.bias', 'Transformer.blocks.10.norm2.weight', 'Transformer.blocks.10.mlp_channels.fc1.bias', 'Transformer.blocks.10.mlp_channels.fc1.weight', 'Transformer.blocks.10.mlp_channels.fc2.bias', 'Transformer.blocks.10.mlp_channels.fc2.weight', 'Transformer.blocks.10.mlp_tokens.fc1.bias', 'Transformer.blocks.10.mlp_tokens.fc1.weight', 'Transformer.blocks.10.mlp_tokens.fc2.bias', 'Transformer.blocks.10.mlp_tokens.fc2.weight', 'Transformer.blocks.11.norm1.bias', 'Transformer.blocks.11.norm1.weight', 'Transformer.blocks.11.norm2.bias', 'Transformer.blocks.11.norm2.weight', 'Transformer.blocks.11.mlp_channels.fc1.bias', 'Transformer.blocks.11.mlp_channels.fc1.weight', 'Transformer.blocks.11.mlp_channels.fc2.bias', 'Transformer.blocks.11.mlp_channels.fc2.weight', 'Transformer.blocks.11.mlp_tokens.fc1.bias', 'Transformer.blocks.11.mlp_tokens.fc1.weight', 'Transformer.blocks.11.mlp_tokens.fc2.bias', 'Transformer.blocks.11.mlp_tokens.fc2.weight', 'Transformer.blocks.2.norm1.bias', 'Transformer.blocks.2.norm1.weight', 'Transformer.blocks.2.norm2.bias', 'Transformer.blocks.2.norm2.weight', 'Transformer.blocks.2.mlp_channels.fc1.bias', 'Transformer.blocks.2.mlp_channels.fc1.weight', 'Transformer.blocks.2.mlp_channels.fc2.bias', 'Transformer.blocks.2.mlp_channels.fc2.weight', 'Transformer.blocks.2.mlp_tokens.fc1.bias', 'Transformer.blocks.2.mlp_tokens.fc1.weight', 'Transformer.blocks.2.mlp_tokens.fc2.bias', 'Transformer.blocks.2.mlp_tokens.fc2.weight', 'Transformer.blocks.3.norm1.bias', 'Transformer.blocks.3.norm1.weight', 'Transformer.blocks.3.norm2.bias', 'Transformer.blocks.3.norm2.weight', 'Transformer.blocks.3.mlp_channels.fc1.bias', 'Transformer.blocks.3.mlp_channels.fc1.weight', 'Transformer.blocks.3.mlp_channels.fc2.bias', 'Transformer.blocks.3.mlp_channels.fc2.weight', 'Transformer.blocks.3.mlp_tokens.fc1.bias', 'Transformer.blocks.3.mlp_tokens.fc1.weight', 'Transformer.blocks.3.mlp_tokens.fc2.bias', 'Transformer.blocks.3.mlp_tokens.fc2.weight', 'Transformer.blocks.4.norm1.bias', 'Transformer.blocks.4.norm1.weight', 'Transformer.blocks.4.norm2.bias', 'Transformer.blocks.4.norm2.weight', 'Transformer.blocks.4.mlp_channels.fc1.bias', 'Transformer.blocks.4.mlp_channels.fc1.weight', 'Transformer.blocks.4.mlp_channels.fc2.bias', 'Transformer.blocks.4.mlp_channels.fc2.weight', 'Transformer.blocks.4.mlp_tokens.fc1.bias', 'Transformer.blocks.4.mlp_tokens.fc1.weight', 'Transformer.blocks.4.mlp_tokens.fc2.bias', 'Transformer.blocks.4.mlp_tokens.fc2.weight', 'Transformer.blocks.5.norm1.bias', 'Transformer.blocks.5.norm1.weight', 'Transformer.blocks.5.norm2.bias', 'Transformer.blocks.5.norm2.weight', 'Transformer.blocks.5.mlp_channels.fc1.bias', 'Transformer.blocks.5.mlp_channels.fc1.weight', 'Transformer.blocks.5.mlp_channels.fc2.bias', 'Transformer.blocks.5.mlp_channels.fc2.weight', 'Transformer.blocks.5.mlp_tokens.fc1.bias', 'Transformer.blocks.5.mlp_tokens.fc1.weight', 'Transformer.blocks.5.mlp_tokens.fc2.bias', 'Transformer.blocks.5.mlp_tokens.fc2.weight', 'Transformer.blocks.6.norm1.bias', 'Transformer.blocks.6.norm1.weight', 'Transformer.blocks.6.norm2.bias', 'Transformer.blocks.6.norm2.weight', 'Transformer.blocks.6.mlp_channels.fc1.bias', 'Transformer.blocks.6.mlp_channels.fc1.weight', 'Transformer.blocks.6.mlp_channels.fc2.bias', 'Transformer.blocks.6.mlp_channels.fc2.weight', 'Transformer.blocks.6.mlp_tokens.fc1.bias', 'Transformer.blocks.6.mlp_tokens.fc1.weight', 'Transformer.blocks.6.mlp_tokens.fc2.bias', 'Transformer.blocks.6.mlp_tokens.fc2.weight', 'Transformer.blocks.7.norm1.bias', 'Transformer.blocks.7.norm1.weight', 'Transformer.blocks.7.norm2.bias', 'Transformer.blocks.7.norm2.weight', 'Transformer.blocks.7.mlp_channels.fc1.bias', 'Transformer.blocks.7.mlp_channels.fc1.weight', 'Transformer.blocks.7.mlp_channels.fc2.bias', 'Transformer.blocks.7.mlp_channels.fc2.weight', 'Transformer.blocks.7.mlp_tokens.fc1.bias', 'Transformer.blocks.7.mlp_tokens.fc1.weight', 'Transformer.blocks.7.mlp_tokens.fc2.bias', 'Transformer.blocks.7.mlp_tokens.fc2.weight', 'Transformer.blocks.8.norm1.bias', 'Transformer.blocks.8.norm1.weight', 'Transformer.blocks.8.norm2.bias', 'Transformer.blocks.8.norm2.weight', 'Transformer.blocks.8.mlp_channels.fc1.bias', 'Transformer.blocks.8.mlp_channels.fc1.weight', 'Transformer.blocks.8.mlp_channels.fc2.bias', 'Transformer.blocks.8.mlp_channels.fc2.weight', 'Transformer.blocks.8.mlp_tokens.fc1.bias', 'Transformer.blocks.8.mlp_tokens.fc1.weight', 'Transformer.blocks.8.mlp_tokens.fc2.bias', 'Transformer.blocks.8.mlp_tokens.fc2.weight', 'Transformer.blocks.9.norm1.bias', 'Transformer.blocks.9.norm1.weight', 'Transformer.blocks.9.norm2.bias', 'Transformer.blocks.9.norm2.weight', 'Transformer.blocks.9.mlp_channels.fc1.bias', 'Transformer.blocks.9.mlp_channels.fc1.weight', 'Transformer.blocks.9.mlp_channels.fc2.bias', 'Transformer.blocks.9.mlp_channels.fc2.weight', 'Transformer.blocks.9.mlp_tokens.fc1.bias', 'Transformer.blocks.9.mlp_tokens.fc1.weight', 'Transformer.blocks.9.mlp_tokens.fc2.bias', 'Transformer.blocks.9.mlp_tokens.fc2.weight'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb996082-0b97-48fb-bad4-1a534cc1382b",
   "metadata": {},
   "source": [
    "# basically just mlp_tokens at this point\n",
    "## everything new i add though imma have to train myself cuz no weights :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e6f330c7-2481-453a-8080-51b84ca214b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.blocks.0.MHSA.project.weight\", \"Transformer.blocks.0.MHSA.project.bias\", \"Transformer.blocks.1.MHSA.project.weight\", \"Transformer.blocks.1.MHSA.project.bias\", \"Transformer.blocks.2.MHSA.project.weight\", \"Transformer.blocks.2.MHSA.project.bias\", \"Transformer.blocks.3.MHSA.project.weight\", \"Transformer.blocks.3.MHSA.project.bias\", \"Transformer.blocks.4.MHSA.project.weight\", \"Transformer.blocks.4.MHSA.project.bias\", \"Transformer.blocks.5.MHSA.project.weight\", \"Transformer.blocks.5.MHSA.project.bias\", \"Transformer.blocks.6.MHSA.project.weight\", \"Transformer.blocks.6.MHSA.project.bias\", \"Transformer.blocks.7.MHSA.project.weight\", \"Transformer.blocks.7.MHSA.project.bias\", \"Transformer.blocks.8.MHSA.project.weight\", \"Transformer.blocks.8.MHSA.project.bias\", \"Transformer.blocks.9.MHSA.project.weight\", \"Transformer.blocks.9.MHSA.project.bias\", \"Transformer.blocks.10.MHSA.project.weight\", \"Transformer.blocks.10.MHSA.project.bias\", \"Transformer.blocks.11.MHSA.project.weight\", \"Transformer.blocks.11.MHSA.project.bias\", \"mlp_head.weight\", \"mlp_head.bias\". \n\tUnexpected key(s) in state_dict: \"head.bias\", \"head.weight\", \"stem.proj.bias\", \"stem.proj.weight\", \"Transformer.blocks.\", \"Transformer.blocks.0.mlp_tokens.fc1.bias\", \"Transformer.blocks.0.mlp_tokens.fc1.weight\", \"Transformer.blocks.0.mlp_tokens.fc2.bias\", \"Transformer.blocks.0.mlp_tokens.fc2.weight\", \"Transformer.blocks.1.mlp_tokens.fc1.bias\", \"Transformer.blocks.1.mlp_tokens.fc1.weight\", \"Transformer.blocks.1.mlp_tokens.fc2.bias\", \"Transformer.blocks.1.mlp_tokens.fc2.weight\", \"Transformer.blocks.2.mlp_tokens.fc1.bias\", \"Transformer.blocks.2.mlp_tokens.fc1.weight\", \"Transformer.blocks.2.mlp_tokens.fc2.bias\", \"Transformer.blocks.2.mlp_tokens.fc2.weight\", \"Transformer.blocks.3.mlp_tokens.fc1.bias\", \"Transformer.blocks.3.mlp_tokens.fc1.weight\", \"Transformer.blocks.3.mlp_tokens.fc2.bias\", \"Transformer.blocks.3.mlp_tokens.fc2.weight\", \"Transformer.blocks.4.mlp_tokens.fc1.bias\", \"Transformer.blocks.4.mlp_tokens.fc1.weight\", \"Transformer.blocks.4.mlp_tokens.fc2.bias\", \"Transformer.blocks.4.mlp_tokens.fc2.weight\", \"Transformer.blocks.5.mlp_tokens.fc1.bias\", \"Transformer.blocks.5.mlp_tokens.fc1.weight\", \"Transformer.blocks.5.mlp_tokens.fc2.bias\", \"Transformer.blocks.5.mlp_tokens.fc2.weight\", \"Transformer.blocks.6.mlp_tokens.fc1.bias\", \"Transformer.blocks.6.mlp_tokens.fc1.weight\", \"Transformer.blocks.6.mlp_tokens.fc2.bias\", \"Transformer.blocks.6.mlp_tokens.fc2.weight\", \"Transformer.blocks.7.mlp_tokens.fc1.bias\", \"Transformer.blocks.7.mlp_tokens.fc1.weight\", \"Transformer.blocks.7.mlp_tokens.fc2.bias\", \"Transformer.blocks.7.mlp_tokens.fc2.weight\", \"Transformer.blocks.8.mlp_tokens.fc1.bias\", \"Transformer.blocks.8.mlp_tokens.fc1.weight\", \"Transformer.blocks.8.mlp_tokens.fc2.bias\", \"Transformer.blocks.8.mlp_tokens.fc2.weight\", \"Transformer.blocks.9.mlp_tokens.fc1.bias\", \"Transformer.blocks.9.mlp_tokens.fc1.weight\", \"Transformer.blocks.9.mlp_tokens.fc2.bias\", \"Transformer.blocks.9.mlp_tokens.fc2.weight\", \"Transformer.blocks.10.mlp_tokens.fc1.bias\", \"Transformer.blocks.10.mlp_tokens.fc1.weight\", \"Transformer.blocks.10.mlp_tokens.fc2.bias\", \"Transformer.blocks.10.mlp_tokens.fc2.weight\", \"Transformer.blocks.11.mlp_tokens.fc1.bias\", \"Transformer.blocks.11.mlp_tokens.fc1.weight\", \"Transformer.blocks.11.mlp_tokens.fc2.bias\", \"Transformer.blocks.11.mlp_tokens.fc2.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [154]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\peep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"patch_encoding.weight\", \"patch_encoding.bias\", \"positional_embedding.pos_embedding\", \"Transformer.blocks.0.MHSA.project.weight\", \"Transformer.blocks.0.MHSA.project.bias\", \"Transformer.blocks.1.MHSA.project.weight\", \"Transformer.blocks.1.MHSA.project.bias\", \"Transformer.blocks.2.MHSA.project.weight\", \"Transformer.blocks.2.MHSA.project.bias\", \"Transformer.blocks.3.MHSA.project.weight\", \"Transformer.blocks.3.MHSA.project.bias\", \"Transformer.blocks.4.MHSA.project.weight\", \"Transformer.blocks.4.MHSA.project.bias\", \"Transformer.blocks.5.MHSA.project.weight\", \"Transformer.blocks.5.MHSA.project.bias\", \"Transformer.blocks.6.MHSA.project.weight\", \"Transformer.blocks.6.MHSA.project.bias\", \"Transformer.blocks.7.MHSA.project.weight\", \"Transformer.blocks.7.MHSA.project.bias\", \"Transformer.blocks.8.MHSA.project.weight\", \"Transformer.blocks.8.MHSA.project.bias\", \"Transformer.blocks.9.MHSA.project.weight\", \"Transformer.blocks.9.MHSA.project.bias\", \"Transformer.blocks.10.MHSA.project.weight\", \"Transformer.blocks.10.MHSA.project.bias\", \"Transformer.blocks.11.MHSA.project.weight\", \"Transformer.blocks.11.MHSA.project.bias\", \"mlp_head.weight\", \"mlp_head.bias\". \n\tUnexpected key(s) in state_dict: \"head.bias\", \"head.weight\", \"stem.proj.bias\", \"stem.proj.weight\", \"Transformer.blocks.\", \"Transformer.blocks.0.mlp_tokens.fc1.bias\", \"Transformer.blocks.0.mlp_tokens.fc1.weight\", \"Transformer.blocks.0.mlp_tokens.fc2.bias\", \"Transformer.blocks.0.mlp_tokens.fc2.weight\", \"Transformer.blocks.1.mlp_tokens.fc1.bias\", \"Transformer.blocks.1.mlp_tokens.fc1.weight\", \"Transformer.blocks.1.mlp_tokens.fc2.bias\", \"Transformer.blocks.1.mlp_tokens.fc2.weight\", \"Transformer.blocks.2.mlp_tokens.fc1.bias\", \"Transformer.blocks.2.mlp_tokens.fc1.weight\", \"Transformer.blocks.2.mlp_tokens.fc2.bias\", \"Transformer.blocks.2.mlp_tokens.fc2.weight\", \"Transformer.blocks.3.mlp_tokens.fc1.bias\", \"Transformer.blocks.3.mlp_tokens.fc1.weight\", \"Transformer.blocks.3.mlp_tokens.fc2.bias\", \"Transformer.blocks.3.mlp_tokens.fc2.weight\", \"Transformer.blocks.4.mlp_tokens.fc1.bias\", \"Transformer.blocks.4.mlp_tokens.fc1.weight\", \"Transformer.blocks.4.mlp_tokens.fc2.bias\", \"Transformer.blocks.4.mlp_tokens.fc2.weight\", \"Transformer.blocks.5.mlp_tokens.fc1.bias\", \"Transformer.blocks.5.mlp_tokens.fc1.weight\", \"Transformer.blocks.5.mlp_tokens.fc2.bias\", \"Transformer.blocks.5.mlp_tokens.fc2.weight\", \"Transformer.blocks.6.mlp_tokens.fc1.bias\", \"Transformer.blocks.6.mlp_tokens.fc1.weight\", \"Transformer.blocks.6.mlp_tokens.fc2.bias\", \"Transformer.blocks.6.mlp_tokens.fc2.weight\", \"Transformer.blocks.7.mlp_tokens.fc1.bias\", \"Transformer.blocks.7.mlp_tokens.fc1.weight\", \"Transformer.blocks.7.mlp_tokens.fc2.bias\", \"Transformer.blocks.7.mlp_tokens.fc2.weight\", \"Transformer.blocks.8.mlp_tokens.fc1.bias\", \"Transformer.blocks.8.mlp_tokens.fc1.weight\", \"Transformer.blocks.8.mlp_tokens.fc2.bias\", \"Transformer.blocks.8.mlp_tokens.fc2.weight\", \"Transformer.blocks.9.mlp_tokens.fc1.bias\", \"Transformer.blocks.9.mlp_tokens.fc1.weight\", \"Transformer.blocks.9.mlp_tokens.fc2.bias\", \"Transformer.blocks.9.mlp_tokens.fc2.weight\", \"Transformer.blocks.10.mlp_tokens.fc1.bias\", \"Transformer.blocks.10.mlp_tokens.fc1.weight\", \"Transformer.blocks.10.mlp_tokens.fc2.bias\", \"Transformer.blocks.10.mlp_tokens.fc2.weight\", \"Transformer.blocks.11.mlp_tokens.fc1.bias\", \"Transformer.blocks.11.mlp_tokens.fc1.weight\", \"Transformer.blocks.11.mlp_tokens.fc2.bias\", \"Transformer.blocks.11.mlp_tokens.fc2.weight\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7c4d80a0-ab58-4906-8966-da7b0f135252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['blocks.0.norm1.bias', 'blocks.0.norm1.weight', 'blocks.0.norm2.bias', 'blocks.0.norm2.weight', 'blocks.0.mlp_channels.fc1.bias', 'blocks.0.mlp_channels.fc1.weight', 'blocks.0.mlp_channels.fc2.bias', 'blocks.0.mlp_channels.fc2.weight', 'blocks.0.mlp_tokens.fc1.bias', 'blocks.0.mlp_tokens.fc1.weight', 'blocks.0.mlp_tokens.fc2.bias', 'blocks.0.mlp_tokens.fc2.weight', 'blocks.1.norm1.bias', 'blocks.1.norm1.weight', 'blocks.1.norm2.bias', 'blocks.1.norm2.weight', 'blocks.1.mlp_channels.fc1.bias', 'blocks.1.mlp_channels.fc1.weight', 'blocks.1.mlp_channels.fc2.bias', 'blocks.1.mlp_channels.fc2.weight', 'blocks.1.mlp_tokens.fc1.bias', 'blocks.1.mlp_tokens.fc1.weight', 'blocks.1.mlp_tokens.fc2.bias', 'blocks.1.mlp_tokens.fc2.weight', 'blocks.10.norm1.bias', 'blocks.10.norm1.weight', 'blocks.10.norm2.bias', 'blocks.10.norm2.weight', 'blocks.10.mlp_channels.fc1.bias', 'blocks.10.mlp_channels.fc1.weight', 'blocks.10.mlp_channels.fc2.bias', 'blocks.10.mlp_channels.fc2.weight', 'blocks.10.mlp_tokens.fc1.bias', 'blocks.10.mlp_tokens.fc1.weight', 'blocks.10.mlp_tokens.fc2.bias', 'blocks.10.mlp_tokens.fc2.weight', 'blocks.11.norm1.bias', 'blocks.11.norm1.weight', 'blocks.11.norm2.bias', 'blocks.11.norm2.weight', 'blocks.11.mlp_channels.fc1.bias', 'blocks.11.mlp_channels.fc1.weight', 'blocks.11.mlp_channels.fc2.bias', 'blocks.11.mlp_channels.fc2.weight', 'blocks.11.mlp_tokens.fc1.bias', 'blocks.11.mlp_tokens.fc1.weight', 'blocks.11.mlp_tokens.fc2.bias', 'blocks.11.mlp_tokens.fc2.weight', 'blocks.2.norm1.bias', 'blocks.2.norm1.weight', 'blocks.2.norm2.bias', 'blocks.2.norm2.weight', 'blocks.2.mlp_channels.fc1.bias', 'blocks.2.mlp_channels.fc1.weight', 'blocks.2.mlp_channels.fc2.bias', 'blocks.2.mlp_channels.fc2.weight', 'blocks.2.mlp_tokens.fc1.bias', 'blocks.2.mlp_tokens.fc1.weight', 'blocks.2.mlp_tokens.fc2.bias', 'blocks.2.mlp_tokens.fc2.weight', 'blocks.3.norm1.bias', 'blocks.3.norm1.weight', 'blocks.3.norm2.bias', 'blocks.3.norm2.weight', 'blocks.3.mlp_channels.fc1.bias', 'blocks.3.mlp_channels.fc1.weight', 'blocks.3.mlp_channels.fc2.bias', 'blocks.3.mlp_channels.fc2.weight', 'blocks.3.mlp_tokens.fc1.bias', 'blocks.3.mlp_tokens.fc1.weight', 'blocks.3.mlp_tokens.fc2.bias', 'blocks.3.mlp_tokens.fc2.weight', 'blocks.4.norm1.bias', 'blocks.4.norm1.weight', 'blocks.4.norm2.bias', 'blocks.4.norm2.weight', 'blocks.4.mlp_channels.fc1.bias', 'blocks.4.mlp_channels.fc1.weight', 'blocks.4.mlp_channels.fc2.bias', 'blocks.4.mlp_channels.fc2.weight', 'blocks.4.mlp_tokens.fc1.bias', 'blocks.4.mlp_tokens.fc1.weight', 'blocks.4.mlp_tokens.fc2.bias', 'blocks.4.mlp_tokens.fc2.weight', 'blocks.5.norm1.bias', 'blocks.5.norm1.weight', 'blocks.5.norm2.bias', 'blocks.5.norm2.weight', 'blocks.5.mlp_channels.fc1.bias', 'blocks.5.mlp_channels.fc1.weight', 'blocks.5.mlp_channels.fc2.bias', 'blocks.5.mlp_channels.fc2.weight', 'blocks.5.mlp_tokens.fc1.bias', 'blocks.5.mlp_tokens.fc1.weight', 'blocks.5.mlp_tokens.fc2.bias', 'blocks.5.mlp_tokens.fc2.weight', 'blocks.6.norm1.bias', 'blocks.6.norm1.weight', 'blocks.6.norm2.bias', 'blocks.6.norm2.weight', 'blocks.6.mlp_channels.fc1.bias', 'blocks.6.mlp_channels.fc1.weight', 'blocks.6.mlp_channels.fc2.bias', 'blocks.6.mlp_channels.fc2.weight', 'blocks.6.mlp_tokens.fc1.bias', 'blocks.6.mlp_tokens.fc1.weight', 'blocks.6.mlp_tokens.fc2.bias', 'blocks.6.mlp_tokens.fc2.weight', 'blocks.7.norm1.bias', 'blocks.7.norm1.weight', 'blocks.7.norm2.bias', 'blocks.7.norm2.weight', 'blocks.7.mlp_channels.fc1.bias', 'blocks.7.mlp_channels.fc1.weight', 'blocks.7.mlp_channels.fc2.bias', 'blocks.7.mlp_channels.fc2.weight', 'blocks.7.mlp_tokens.fc1.bias', 'blocks.7.mlp_tokens.fc1.weight', 'blocks.7.mlp_tokens.fc2.bias', 'blocks.7.mlp_tokens.fc2.weight', 'blocks.8.norm1.bias', 'blocks.8.norm1.weight', 'blocks.8.norm2.bias', 'blocks.8.norm2.weight', 'blocks.8.mlp_channels.fc1.bias', 'blocks.8.mlp_channels.fc1.weight', 'blocks.8.mlp_channels.fc2.bias', 'blocks.8.mlp_channels.fc2.weight', 'blocks.8.mlp_tokens.fc1.bias', 'blocks.8.mlp_tokens.fc1.weight', 'blocks.8.mlp_tokens.fc2.bias', 'blocks.8.mlp_tokens.fc2.weight', 'blocks.9.norm1.bias', 'blocks.9.norm1.weight', 'blocks.9.norm2.bias', 'blocks.9.norm2.weight', 'blocks.9.mlp_channels.fc1.bias', 'blocks.9.mlp_channels.fc1.weight', 'blocks.9.mlp_channels.fc2.bias', 'blocks.9.mlp_channels.fc2.weight', 'blocks.9.mlp_tokens.fc1.bias', 'blocks.9.mlp_tokens.fc1.weight', 'blocks.9.mlp_tokens.fc2.bias', 'blocks.9.mlp_tokens.fc2.weight', 'head.bias', 'head.weight', 'norm.bias', 'norm.weight', 'stem.proj.bias', 'stem.proj.weight'])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys() # MLP TOKENS MLP TOKENS MLP TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca7218-1a44-4f57-817b-6326a1b70655",
   "metadata": {},
   "source": [
    "# BRO I CANT EVEN FIND MLP_TOKENS ON THIS DUDES GITHUB WTF IS AN MLP TOKEN\n",
    "## ... >:("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b681af-8b3d-40b2-a57c-6f263f816706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
