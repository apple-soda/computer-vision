{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a88578-109a-4a23-b0f6-9308c383afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f03bb4d-b2b3-47d7-963f-d11f58b6eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b92913-44ee-4347-acd3-22eafde98515",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb5f686-b3e4-47eb-ab7b-c7eefac01c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module): # pretty overkill, but need this structure in order to load image-models weights\n",
    "    def __init__(self, patch_size, in_channels, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels=in_channels, out_channels=dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.proj(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, dim, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "class EncoderBlock(nn.Module): # layerscale, droppath?\n",
    "    def __init__(self, dim, ff_dim, num_heads, dropout, attention_dropout):\n",
    "        super().__init__()\n",
    "        # layer norm -> mhsa -> layer norm -> mlp\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = MultiheadAttention(dim, num_heads, dropout, attention_dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mlp = MLP(dim, ff_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        out = x + self.mlp(self.norm2(x))\n",
    "        return out\n",
    "\n",
    "    \n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout, attention_dropout, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        #assert isinstance(self.dim/self.num_heads, int)\n",
    "        self.h = self.dim // self.num_heads\n",
    "    \n",
    "        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attention_dropout)\n",
    "        self.proj = nn.Linear(self.dim, self.dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, s, 3, self.num_heads, self.h).permute(2, 0, 3, 1, 4) # [3, batch_size, num_heads, sequence_length, attention_height]\n",
    "        q, k, v = qkv.unbind(0) # q, k, v of shape [b, n, s, h]\n",
    "        \n",
    "        k = k.transpose(-2, -1)\n",
    "        attention = torch.matmul(q, k)\n",
    "        attention = attention * (self.h ** -0.5)\n",
    "        attention = self.attn_drop(F.softmax(attention, dim=-1))\n",
    "        \n",
    "        out = torch.matmul(attention, v) # b, h, s, w\n",
    "        out = out.permute(0, 2, 1, 3) # b, s, h, w \n",
    "        out = out.flatten(2) # b, s, d\n",
    "        \n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79206819-5371-4500-a764-f4f87b03bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, **params):\n",
    "        super().__init__()\n",
    "        # static skeleton\n",
    "        self.input_channels = params['input_channels']\n",
    "        self.dim = params['dim']\n",
    "        self.hidden_dim = params['hidden_dim']\n",
    "        self.patch_size = params['patch_size']\n",
    "        self.img_size = params['img_size']\n",
    "        self.dropout = params['dropout']\n",
    "        self.attention_dropout = params['attention_dropout']\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.num_heads = params['num_heads']\n",
    "        self.num_classes = params['num_classes']\n",
    "        \n",
    "        # static optional\n",
    "        self.ft_classes = params['ft_classes']\n",
    "        self.encoder_norm = params['encoder_norm']\n",
    "        self.fc_norm = params['fc_norm']\n",
    "        \n",
    "        # dynamic\n",
    "        self.embed_height = self.img_size // self.patch_size\n",
    "        self.embed_width = self.img_size // self.patch_size\n",
    "        self.seq_len = self.embed_height * self.embed_width + 1 # add one here for token\n",
    "        \n",
    "        # network structure\n",
    "        self.patch_embed = PatchEmbedding(self.patch_size, 3, self.dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.seq_len, self.dim))\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            EncoderBlock(self.dim, self.hidden_dim, self.num_heads, self.dropout, self.attention_dropout) for i in range(self.num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(self.dim, eps=1e-6) if self.encoder_norm else nn.Identity()\n",
    "        self.fc_norm = nn.LayerNorm(self.dim, eps=1e-6) if self.fc_norm else nn.Identity()\n",
    "        self.head = nn.Linear(self.dim, self.num_classes)\n",
    "        self.ft_layer = nn.Linear(self.num_classes, self.ft_classes) if self.ft_classes else nn.Identity()\n",
    "        \n",
    "    def process_input(self, x):\n",
    "        # inputs are of shape (b, c, h, w)\n",
    "        # outputs should be of shape (b, s, d)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2)\n",
    "        out = x.permute(0, 2, 1)\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b = x.shape[0] # batch size\n",
    "        x = self.process_input(x)\n",
    "        \n",
    "        # concatenate token\n",
    "        token = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat([token, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # extract token\n",
    "        x = x[:, 0]\n",
    "        x = self.fc_norm(x)\n",
    "        x = self.head(x)\n",
    "        out = self.ft_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1decd75d-c60f-4bc7-8377-2f1d585ab6e6",
   "metadata": {},
   "source": [
    "## you can normalize images and shit in here too if u want idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0a48e7-1a43-4796-8e71-834c5629f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_size):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.dir = 'train'\n",
    "        \n",
    "        i = 0\n",
    "        for folder in os.listdir(self.dir):\n",
    "            label = i\n",
    "            for file in tqdm(os.listdir(os.path.join(self.dir, folder))):\n",
    "                img_path = os.path.join(os.path.join(self.dir, folder), file)\n",
    "                \n",
    "                # Open and resize the img\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, image_size)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = np.reshape(image, [3, 224, 224])\n",
    "                 \n",
    "                # Append the image and its corresponding label to the output\n",
    "                self.data.append(image)\n",
    "                self.labels.append(label)\n",
    "            i += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7c6dd-673d-46e7-a18f-e089ac30d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3587/3587 [00:27<00:00, 130.56it/s]\n",
      "100%|██████████████████████████████████████████████| 2215/2215 [00:17<00:00, 124.49it/s]\n",
      "100%|██████████████████████████████████████████████| 4518/4518 [00:36<00:00, 124.26it/s]\n",
      "100%|██████████████████████████████████████████████| 1493/1493 [00:11<00:00, 125.42it/s]\n",
      "100%|██████████████████████████████████████████████| 1751/1751 [00:13<00:00, 127.90it/s]\n",
      " 62%|████████████████████████████▋                 | 4477/7165 [00:38<00:24, 108.17it/s]"
     ]
    }
   ],
   "source": [
    "data = Data((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2a412-1edf-471b-876a-051c70332a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[0][0].reshape(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaff4b6-09aa-46ad-95ce-5fdc30cbc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51475c9-0e68-4f09-8a77-b631e6ce36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "weights = timm.create_model('vit_base_patch16_224_in21k', pretrained=True, num_classes=8)\n",
    "weights = weights.to(device)\n",
    "torch.save(weights.state_dict(), 'alexguo.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf36106-2a3a-45b0-9217-56dfd5f64d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'input_channels':3, 'dim':768, 'hidden_dim':3072, 'patch_size':16, 'img_size':224, 'num_layers':12, 'ft_classes': None, \n",
    "          'dropout':0.1, 'attention_dropout':0.1, 'num_heads':12, 'num_classes':8, 'encoder_norm':True, 'fc_norm':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87af81-d563-4cb1-82cc-241bc5b43e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(**params)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('alexguo.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89074d4-66aa-408b-8ae5-c928b3c97824",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6fb88-8818-49f4-b4f9-f3d135e471d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7e832-329b-4689-9683-c54fb363501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = torch.utils.data.random_split(data, [22972, 2552])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001b547-696e-40e1-a11a-b7854e6409c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change whatever params u need i just have 16 batch size cuz my gpu memory is bad\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=16, num_workers=8, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val, batch_size=16, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f842443-3da3-4732-8673-f68fef22c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.tensor(data[0][0], device=device, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20720425-4a6c-4b66-84d6-de4550c5430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kek = model(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df140d5b-7807-4f7f-88e6-fe1815d5d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(kek) # dumb ass vit lmfao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eebfc-2021-479c-933a-a6a1ac33c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tloss, avg_vloss = [], []\n",
    "\n",
    "for e in range(100):\n",
    "    t_loss, v_loss = [], []\n",
    "    for x, y in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x.float())\n",
    "        l = loss(y_pred, y)\n",
    "        npl = l.detach().cpu().numpy()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        t_loss.append(npl)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "        for x, y in valloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x.float())\n",
    "            l = loss(y_pred, y)\n",
    "            npl = l.detach().cpu().numpy()\n",
    "            v_loss.append(npl)\n",
    "    \n",
    "    avg_tloss.append(np.mean(t_loss))\n",
    "    avg_vloss.append(np.mean(v_loss))\n",
    "    \n",
    "    print(f'Epoch: {e}, Training Loss: {np.mean(t_loss)}, Validation Loss: {np.mean(v_loss)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
